# PySpark-Big-Data-Project-to-Learn-RDD-Operations

# Business Overview

Apache Spark is a distributed processing engine that is open source and used for large data applications. It uses in-memory caching and efficient query 
execution for quick analytic queries against any quantity of data. It offers code reuse across many workloads such as batch processing, interactive 
queries, real-time analytics, machine learning, and graph processing. It provides development APIs in Java, Scala, Python, and R.

# Data Pipeline:

A data pipeline is a technique for transferring data from one system to another. The data may or may not be updated, and it may be handled in 
real-time (or streaming) rather than in batches. The data pipeline encompasses everything from harvesting or acquiring data using various methods to storing 
raw data, cleaning, validating, and transforming data into a query-worthy format, displaying KPIs, and managing the above process.

Agenda:

This is the second project in the Pyspark series. The first project involves the Pyspark introduction, Spark component and architecture, and basic 
introduction about RDD and DAG. This project involves in-depth knowledge of RDD, different types of RDD operations, the difference between transformation 
and action, and the various functions available in transformation and action with their execution.

 

Tech stack:  

PySpark is a Python interface for Apache Spark. It not only lets you develop Spark applications using Python APIs, but it also includes the PySpark 
shell for interactively examining data in a distributed context. PySpark supports most of Spark's capabilities, including Spark SQL, DataFrame, Streaming, 
MLlib, and Spark Core. In this project, you will learn about core Spark architecture, Spark Sessions, Transformation, Actions, and Optimization Techniques using PySpark.

 
